### Analysis of Representations for Domain Adaptation
###### published: 2006
###### authors:  Shai Ben-David University of Waterloo

> Intuitively, a good feature representation is a crucial factor in the success of domain adaptation.

> In this situation, we expect the task performance in the target domain to depend on both the performance in the source domain and the similarity between the two domains.

> Our learning setting is defined by fixed but unknown D and f, and our choice of representation function R and hypothesis class $H \in \lbrace {g:z \rightarrow \lbrace {0,1}} \rbrace \rbrace$ of deterministic hypotheses to be used to approximate the function $f$.

> We denote by $D_S$ the source distribution of instances and $\tilde{D_S}$ the induced distribution over the feature space $Z$. We use parallel notation, $D_T$, $\tilde{D_T}$, for the target domain. $f: X \rightarrow [0,1]$ is the labeling rule, common to both domains, and $\tilde{f}$ is the indeced image of f under $R$.

> A predictor is a function, h, from the feature space, $Z$ to [0,1].

$$
\epsilon_S(h) = E_{z \sim \tilde{D_S}}|\tilde{f(z)} - h(z)|
$$

**The A-distance**
> Given a domain $X$ and a collection $A$ of subsets of $X$, let $D$, $D^1$ be probability distribution over $X$, such that every set in $A$ is measurable with respect to both distributions, the $A$-distance between such distribution is defined as:

$$
d_A(D, \tilde{D}) = 2 \sup_{a \in A}|Pr_D[A] - Pr_{\tilde{D}}[A]|
$$

> We say that a function $\tilde{f}:Z \rightarrow [0,1]$ is $\lambda$-close to a function class $H$ with respect to distributions $D_S$ and $D_T$ if
$$
\inf_{h \in H}[\epsilon_S(h) + \epsilon_T(h)] \leq \lambda
$$
> **A function $\tilde{f}$ is $\lambda$-close to $H$ when there is a single hypothesis $h \in H$ which performs well on both domains.**

> The correspnodence between functions and characteristic subsets. For a binary-valued function $g(z)$, we let $Z_g \in Z$ be the subset whose characteristic function is g:

$$
Z_g = \lbrace {z\in Z}:g(z)=1 \rbrace
$$

**Recalling the relationship between sets and their characteristic functions, it should be clear that computing the A-distance is closely related to learning a classifier. In fact they are identical. The set $A_h \in H$ which maximizes the $H$-distance between $\tilde{D_S}$ and $\tilde{D_T}$ has a characteristic function h. Then h is the classifier which achieves minimum error on the binary classification problem of discriminating between points generated by the two distributions.**
